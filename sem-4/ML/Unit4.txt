Q1. Which variables should we include in a regression model? 
    1. Dependent variable (Y):
       - The outcome you want to predict.
    2. Independant variable (X)
       - The predictors or features that influence (affect) the dependent variable.
    3. Control variable 
       - Account for other influencing factors
    4. Dummy variables -
       - To represent categorical data.   
    5. Interaction terms
       - Joint effects of variables.

###########################################################################################
Q2. Explain VIF (Variance Inflation Factor).
    1. VIF - Stand fro Variance INflation Factor
    2. Defincation - Varience Inflation Fator (VIF) measure how much predictor varience increased with corelation with other predictor.

    VIF Conditions
        1. VIF = 1
          - when VIF = 1 , NO correlation with other variables.
        2. VIF > 1
          - When VIF is greater than one , Correlation with other variables.
        3. VIF > 5 or 10
          - when VIF greater than 5 or 10, Indicate problematic multicollinearity,
            which may require action like removing or combining variables.

#################################################################################################
Q3. What are the impacts on regression model?
    1. High standard Errors    
       - Hard to find important predictors
    2. Unstable coefficient
       - Unstable coefficient can lead inconsistent result.
    3. Hard to Understand 
       - Hard to understand each predictor effects
    4. Overfitting
       - Model may overfit while working with new data.
    5. Wrong conclusion
       - Mislead result and insight.

#####################################################################################################
Q7.Explain the Akaike information criterion (AIC)
  1. AIC stand for Akaike information criterison
  2. Akaike information criterison is a measeure to compare diffrent models
  3. It help to balance model fit and complexity
  4. AIC goal is to find the model which fit the data well and which is less complex.

  Formula -
          AIC =  2k − 2ln (L)

  Conditions 
    1. Lower AIC - Indicate A better model, with less complex 
    2. Higher AIC - Indicate more complex model and can overfit data.

################################################################################################
Q8. What is polynomial linear regression.
Q10.What is the polynomial regression is non – local

    1. Defination - Polynominal Linear Regression fit curve to data by adding powers of predictors like ( x3, x4 )
    2. For Example -

       Linear Y = B0 + B1X    # down power
       Polynomial Y = B0 + B1X + B2X2 + B2X3 ....  

    3. Polynomial Regression is non-local beacuse they uses higher degree terms 
       eg - x3 , x4
       Due to this model allow to find out global trends not just local trends

##############################################################################################################
Q9. What is the physical significance of coefficients.
   1. Scaling Factor -
      show how one thing affect another
      example F = ma  //  here m shows how much force is needed for a given acceleration.

    2. Physical Properties
       - Represent characterstic of material
       example  F = kx   // here k is spring stiffness

    3. Rate/constant-
       - Tell how fast something happen
       example:  d = v x t   // distance = velocity * time
       here d show rate of change of distnce with time.

    4. Unit converter
       - Keep equeation correct in Unit
        Example - KE = 1/2 mv2    here 1/2 adjust the units

####################################################################################################
Q4. What is the stratification technique and explain with example.
   1. Stratification is a quality control tool used to separate data into different groups 
      to understand patterns or problems more clearly. 
   2. It help you to find the real cause of problem.
   3. Stratification = Sorting data into groups to find hidden problems.

   Examle -
      A company find defect in his Products, to find cause of it, 
      - Its stratifies defect data by shift

            Shift                     Defect Found
     -----------------------------------------------------       
            Morning          |            2
            Afternoon        |            3
            Night            |            12
     -----------------------------------------------------

      Analysis - 
          Most Defect happen in Night shift.
         
      Now, the company knows where to investigate further.

#########################################################################################################
# Q1 Write a note on SLR.

  Concept -
   1. Multiple Linear Regression is used to predict a dependent variable (Y) from two or more independent variables 
     (X₁, X₂, ..., Xn).
     

   2. It assumes a linear relationship between the dependent variable and all independent variables.
   3. B₀, B₁, B₂, ..., Bn are the main parameters of multiple linear regression.

   Equation 
       Y = B₀ + B₁X₁ + B₂X₂ + ... + BnXn

       Where,
         Y = Dependant Varable    (what you are goint to predict)
         X₁, X₂, ..., Xn    =    Independent Variables  (Input or feature)
         B₀                 =    Intercept         (Value of Y when all x = 0)
         B₁, B₂, ..., Bn    =    Coefficients/Slope           

#########################################################################################################
Q3 What is a PLR ? Explain it with an example.
   1. Polynomial Linear Regression is used to model a non-linear relationship 
      between the dependent variable (Y) and the independent variable (X).
   2. It transforms the input features into higher-degree polynomial terms (like X², X³, etc.) 
   3. Althought the relastionship is non-linear but still the PLR are considered as type of Linear Model
       because it is linear in the coefficients (B₀, B₁, B₂, ...).


   Equation -
    Y = B₀ + B₁X + B₂X² + B₃X³ + ... + BnXⁿ

   Where:
      Y = Dependent Variable  (what you are going to predict)

      X = Independent Variable  (input or feature)

      X², X³, ..., Xⁿ = Polynomial terms of X

      B₀ = Intercept  (value of Y when X = 0)

      B₁, B₂, ..., Bn = Coefficients for each power of X (control shape of the curve)




      Python Example
      --------------------------------------------------------------

      from sklearn.linear_model import LinearRegression
      from sklearn.preprocessing import PolynomialFeatures
      import numpy as np

      X = [[1], [2], [3]]
      y = [1, 4, 9]

      X_poly = PolynomialFeatures(degree=2).fit_transform(X)
      model = LinearRegression().fit(X_poly, y)
      print(model.predict(X_poly))


#############################################################################################################
Q7 What are the pros and cons of decision tree model?
Q6 How decision tree is used for regression?

    Defination 
       - A decision tree is a model used to make decisions by splitting data based on features, forming a tree structure
   
    Advantage -
      1. Easy to understand
      2. Data scalling not required
      3. handle both numerical and categorical data.
      4. Can model both linear and non-linear relastionship.
      5. Useful for feature importance analysis


    Disadvantages
      1. Overfitting with complex Trees
      2. Sensitive to small change in data
      3. Can be bias toward dominant classes.


      Working -
        Split the data 
              |
        Nodes test the feature 
              |
        Prediction

###############################################################################################################
Q8 How KNN is used for regression problem?
Q10 Why KNN is called as lazy algorithm?
Q11 How KNN is work?

   1. KNN stands for Kth nearest Neighbors KNN
   2. KNN is simple instance based learning algorithm 
   3. It is used for regression and classifications

   Working -
     1. Find the kth nearest neighbours
        - Given new points , KNN calculate distance from given points to each points
     2. Select K closest point
        - Algorithm find the closest kth point which is nearest to the given point
     3. Make Prediction 
        - Classification - Majority class which are near to k neighbour are predicted class
        - Regressions - The average (mean) of the K neighbors' values is the predicted value.


    KNN called lazy
    4.  It is also know as lazy algorithm
        1. It not have learning phase or steps
        2. It doest build model during Training
           Instead it memoris the entire dataset
        3. Computes Prediction at Runtime

        4. In short KNN does all work while prediction, making it lazy compare to other algorithms.

#########################################################################################################
Q12 What is the limitations of 1-nearest neighbour?
   1. Sensitive to Noisy data 
      - One wrong point can give wrong prediction
   2. Overfitting
      - 1-NN can easily overfit the data, 
        especially in cases with noisy or complex datasets.
   3. Computational Expensive
      - 1-NN is compuatinal expensive as it checks each points for prediction
   4. Slow for large dataset
      - 1-NN is slow as it checks each points for prediction
   
##########################################################################################################

Q9 What is the difference between instance based learning and model based 
learning?

 Instance based Learning  

1.  No training phase                      
2.  Used entire dataset for prediction      
3.  Example - KNN                           
4.  Adapt new data quickly                 
5.  Cannot handle large dataset efficently   
6.  Not memory efficient 
7.  computational expense increase as dataset increase
8.  It is sensitive to noisy data.

Model Based learning

  1. It has training phase
  2. Use model for prediction
  3. Example - SLR, MLR, Decision Tree
  4. Cannot Adapt new data quickly
  5. It can handle large datasets more efficiently.
  6. It is memory efficent save memory
  7. computational expense remain constant.
  8. It is less sensitive to noisy data.

####################################################################################################
Q13 What is the regularization ?why it is need ?
Q14 How we do regularization ?
Q15 What are the types of regularization ? Explain it in detail.
Q19 How many ways that we can used to minimize the overfiting ?

   Regularization - 
      1. Regularization is a technique used to prevent overfitting
        in machine learning models by penalizing complex models.
      2. There are some unusual or unseen data present in model so it downgread the performance of model
         this are overfitting
      3. Regularization helps model to be simpler and more genearl so it perform well in unseen data.

      Types of Regularization
          1. L1 Regularization 
             - L1 Regularization is also know as Lasso
             - Adds the absolute values of weights as a penalty.
             - Makes some weight to zero, so it help to remove unimportant feature
             - help to reduce model complexity
             
          2. L2 Regularization (Ridge)
             -  L2 Regularization is also known as Ridge
             -  Adds the squares of weights as a penalty.
             -  Shrink all weights but keep all features
             - help to reduce model complexity

          3. Elastic Net
             - It is combination of both L1 and L2 Regularization
             - Combines both benifits removes some feature and shrinks others

################################################################################################################
Q25 What is the difference between in ridge and lasso regression ?
            Ridge                           Lasso
1. Ridge uses L2 regularization         1.  Lasso uses L1 Regularization
2. Not shrink weight to zero            2. Shrink weight to zero
3. Ridge keeps all feature              3. Lasso Removes unimportant feature
4. Rige is better when all feature      4. Lasso is better when few features 
   matter                                  are matter
5. Handles multicollinearity well       5. Struggle with handling multicollinearity 
6. give non spare model                 6. give spare model
7. cannot perform feature selection     7. Peform feature selection
8. less interpretable                   8. more interpretable
9. work well when p > n                 9. unstable or fail when p > n 

####################################################################################################################


Q24 How to implement lasso regression using python ?

      from sklearn.linear_model import Lasso

      model = Lasso(alpha=0.1)     # alpha is the regularization strength
      model.fit(X_train, y_train)  # Train the model
      y_pred = model.predict(X_test)  # Make predictions

-------------------------------------------------------------------------------------------------------------
Q20: Ridge Regression in Python 

      from sklearn.linear_model import Ridge

      model = Ridge(alpha=0.1)      # alpha controls the regularization strength
      model.fit(X_train, y_train)   # Train the model
      y_pred = model.predict(X_test)  # Make predictions

------------------------------------------------------------------------------------------------------------------

Q26 How to implement elastic net regression using python ?

      from sklearn.linear_model import ElasticNet

      model = ElasticNet(alpha=0.1, l1_ratio=0.5)  # l1_ratio controls mix of Lasso & Ridge
      model.fit(X_train, y_train)                 # Train the model
      y_pred = model.predict(X_test)    
            
#####################################################################################################################